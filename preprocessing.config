# pre-processing option

data_type = bitext #<string> (accepted: bitext, monotext, feattext; default: bitext)

dry_run = false
#[<boolean>](default: false) If set, this will only prepare the preprocessor. Useful when using file sampling to test distribution rules.

save_data = './bin/bin-train'

#Logger options
log_file = './bin/bin.log'
disable_logs = false
log_level = INFO #<string> (accepted: DEBUG, INFO, WARNING, ERROR, NONE; default: INFO)
seed = 3425

#Data options

train_dir = './data'
train_src = 'train.txt'
train_tgt = 'train.txt'
valid_src = 'val.txt'
valid_tgt = 'val.txt'

src_vocab = ''
src_suffix = .zh #<string> (default: .src) Suffix for source files in train/valid directories.
src_vocab_size = 50000
src_words_min_frequency = 0
#<table> (default: 0) List of source words min frequency: word[ feat1[ feat2[ ...] ] ]. If = 0, vocabularies are pruned by size.
tgt_vocab = ''
tgt_suffix = .ko
tgt_vocab_size = 50000
tgt_words_min_frequency = 0
src_seq_length = 50
tgt_seq_length = 50
check_plength = false #[<boolean>] (default: false) Check source and target have same length (for seq tagging).
features_vocabs_prefix = '' #<string> (default: '') Path prefix to existing features vocabularies.
time_shift_feature = true #[<boolean>] (default: true) Time shift features on the decoder side.
keep_frequency = false #[<boolean>] (default: false)Keep frequency of words in dictionary.

gsample = 0 #<number> (default: 0) If not zero, extract a new sample from the corpus.
#In training mode, file sampling is done at each epoch. Values between 0 and 1 indicate ratio, values higher than 1 indicate data size
gsample_dist = ''
#<string> (default: '') Configuration file with data class distribution to use for sampling training corpus. If not set, sampling is uniform.
sort = true
#[<boolean>] (default: true) If set, sort the sequences by size to build batches without source padding.
shuffle = true
#[<boolean>] (default: true) If set, shuffle the data (prior sorting).
idx_files = false
#[<boolean>] (default: false) If set, source and target files are 'key value' with key match between source and target.

report_progress_every = 100000
#<number> (default: 100000) Report status every this many sentences.
preprocess_pthreads = 8
#<number> (default: 4) Number of parallel threads for preprocessing.

#Tokenizer options
tok_{src,tgt}_mode = space
#Tokenizer options
tok_{src,tgt}_mode = space
#<string> (accepted: conservative, aggressive, space; default: space)
#Define how aggressive should the tokenization be. space is space-tokenization.

tok_{src,tgt}_joiner_annotate = false
#[<boolean>] (default: false)
#Include joiner annotation using -joiner character.

tok_{src,tgt}_joiner = ￭
#<string> (default: ￭)
#Character used to annotate joiners.

tok_{src,tgt}_joiner_new = false
#[<boolean>] (default: false)
#In -joiner_annotate mode, -joiner is an independent token.

tok_{src,tgt}_case_feature = false
#[<boolean>] (default: false)
#Generate case feature.

tok_{src,tgt}_segment_case = false
#[<boolean>] (default: false)
#Segment case feature, splits AbC to Ab C to be able to restore case

#tok_{src,tgt}_segment_alphabet
#<table> (accepted: Tagalog, Hanunoo, Limbu, Yi, Hebrew, Latin, Devanagari, Thaana, Lao, Sinhala, Georgian, Kannada, Cherokee, Kanbun, Buhid, Malayalam, Han, Thai, Katakana, Telugu, Greek, Myanmar, Armenian, Hangul, Cyrillic, Ethiopic, Tagbanwa, Gurmukhi, Ogham, Khmer, Arabic, Oriya, Hiragana, Mongolian, Kangxi, Syriac, Gujarati, Braille, Bengali, Tamil, Bopomofo, Tibetan)

tok_{src,tgt}_segment_numbers = false
#[<boolean>] (default: false)
#Segment numbers into single digits.

tok_{src,tgt}_segment_alphabet_change = false
#[<boolean>] (default: false)
#Segment if alphabet change between 2 letters.

tok_{src,tgt}_bpe_model = './data/train.tok.bpe.code.10000'
#<string> (default: '')
#Apply Byte Pair Encoding if the BPE model path is given. If the option is used, BPE related options will be overridden/set automatically if the BPE model specified by -bpe_model is learnt using learn_bpe.lua.

tok_{src,tgt}_bpe_EOT_marker = </w>
#<string> (default: </w>)
#Marker used to mark the End of Token while applying BPE in mode 'prefix' or 'both'.

tok_{src,tgt}_bpe_BOT_marker =<w>
#<string> (default: <w>)
#Marker used to mark the Beginning of Token while applying BPE in mode 'suffix' or 'both'.

tok_{src,tgt}_bpe_case_insensitive = false
#[<boolean>] (default: false)
#Apply BPE internally in lowercase, but still output the truecase units. This option will be overridden/set automatically if the BPE model specified by -bpe_model is learnt using learn_bpe.lua.
tok_{src,tgt}_bpe_mode = suffix
#<string> (accepted: suffix, prefix, both, none; default: suffix)
tok_{src,tgt}_bpe_mode = suffix
#<string> (accepted: suffix, prefix, both, none; default: suffix)
#Define the BPE mode. This option will be overridden/set automatically if the BPE model specified by -bpe_model is learnt using learn_bpe.lua. prefix: append -bpe_BOT_marker to the begining of each word to learn prefix-oriented pair statistics; suffix: append -bpe_EOT_marker to the end of each word to learn suffix-oriented pair statistics, as in the original Python script; both: suffix and prefix; none: no suffix nor prefix.
