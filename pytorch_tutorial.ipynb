{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 641.0698852539062\n",
      "1 596.6747436523438\n",
      "2 557.7380981445312\n",
      "3 523.2838134765625\n",
      "4 492.4844055175781\n",
      "5 464.98486328125\n",
      "6 439.8149719238281\n",
      "7 416.69049072265625\n",
      "8 395.1796875\n",
      "9 375.2060241699219\n",
      "10 356.4792175292969\n",
      "11 338.8349304199219\n",
      "12 322.2173156738281\n",
      "13 306.3661193847656\n",
      "14 291.2605285644531\n",
      "15 276.9498291015625\n",
      "16 263.28656005859375\n",
      "17 250.20050048828125\n",
      "18 237.67408752441406\n",
      "19 225.7671356201172\n",
      "20 214.42071533203125\n",
      "21 203.572265625\n",
      "22 193.23353576660156\n",
      "23 183.376953125\n",
      "24 173.96966552734375\n",
      "25 164.99476623535156\n",
      "26 156.42930603027344\n",
      "27 148.25270080566406\n",
      "28 140.4392852783203\n",
      "29 132.94419860839844\n",
      "30 125.80224609375\n",
      "31 119.00061798095703\n",
      "32 112.54032897949219\n",
      "33 106.39104461669922\n",
      "34 100.5414810180664\n",
      "35 94.9902114868164\n",
      "36 89.73754119873047\n",
      "37 84.74893951416016\n",
      "38 79.9990463256836\n",
      "39 75.49420166015625\n",
      "40 71.22856140136719\n",
      "41 67.19147491455078\n",
      "42 63.372955322265625\n",
      "43 59.76323318481445\n",
      "44 56.35694885253906\n",
      "45 53.1414909362793\n",
      "46 50.1088981628418\n",
      "47 47.25053405761719\n",
      "48 44.55931854248047\n",
      "49 42.02329635620117\n",
      "50 39.636314392089844\n",
      "51 37.3880729675293\n",
      "52 35.262168884277344\n",
      "53 33.26425552368164\n",
      "54 31.382577896118164\n",
      "55 29.611656188964844\n",
      "56 27.945491790771484\n",
      "57 26.376239776611328\n",
      "58 24.9006290435791\n",
      "59 23.509815216064453\n",
      "60 22.201824188232422\n",
      "61 20.96882438659668\n",
      "62 19.808818817138672\n",
      "63 18.7175350189209\n",
      "64 17.690242767333984\n",
      "65 16.723764419555664\n",
      "66 15.812411308288574\n",
      "67 14.954290390014648\n",
      "68 14.146561622619629\n",
      "69 13.384522438049316\n",
      "70 12.666482925415039\n",
      "71 11.98994255065918\n",
      "72 11.3526029586792\n",
      "73 10.751133918762207\n",
      "74 10.183491706848145\n",
      "75 9.648231506347656\n",
      "76 9.143255233764648\n",
      "77 8.66722297668457\n",
      "78 8.217146873474121\n",
      "79 7.791935920715332\n",
      "80 7.390427589416504\n",
      "81 7.011647701263428\n",
      "82 6.65355920791626\n",
      "83 6.315211772918701\n",
      "84 5.995817184448242\n",
      "85 5.693788051605225\n",
      "86 5.408263683319092\n",
      "87 5.1376872062683105\n",
      "88 4.881750106811523\n",
      "89 4.639634132385254\n",
      "90 4.410307884216309\n",
      "91 4.1934895515441895\n",
      "92 3.9879820346832275\n",
      "93 3.793234348297119\n",
      "94 3.6085760593414307\n",
      "95 3.4338672161102295\n",
      "96 3.268079996109009\n",
      "97 3.1109135150909424\n",
      "98 2.962043046951294\n",
      "99 2.820622444152832\n",
      "100 2.6864426136016846\n",
      "101 2.5590977668762207\n",
      "102 2.438295364379883\n",
      "103 2.3236451148986816\n",
      "104 2.214708089828491\n",
      "105 2.1112992763519287\n",
      "106 2.0130703449249268\n",
      "107 1.9197689294815063\n",
      "108 1.8312041759490967\n",
      "109 1.7467602491378784\n",
      "110 1.6665314435958862\n",
      "111 1.5903724431991577\n",
      "112 1.5177476406097412\n",
      "113 1.4487435817718506\n",
      "114 1.3830780982971191\n",
      "115 1.320635199546814\n",
      "116 1.2611960172653198\n",
      "117 1.2045984268188477\n",
      "118 1.150708794593811\n",
      "119 1.0993841886520386\n",
      "120 1.0505311489105225\n",
      "121 1.0039335489273071\n",
      "122 0.9595481157302856\n",
      "123 0.9172594547271729\n",
      "124 0.8769084215164185\n",
      "125 0.8384171724319458\n",
      "126 0.8017348647117615\n",
      "127 0.7667273283004761\n",
      "128 0.7333738207817078\n",
      "129 0.701508104801178\n",
      "130 0.6711199283599854\n",
      "131 0.6421259045600891\n",
      "132 0.614434540271759\n",
      "133 0.5880178213119507\n",
      "134 0.5628415942192078\n",
      "135 0.5388164520263672\n",
      "136 0.515864908695221\n",
      "137 0.49396535754203796\n",
      "138 0.47303006052970886\n",
      "139 0.45305806398391724\n",
      "140 0.4339519143104553\n",
      "141 0.4157126247882843\n",
      "142 0.3982814848423004\n",
      "143 0.38159462809562683\n",
      "144 0.3656635880470276\n",
      "145 0.3504115641117096\n",
      "146 0.3358263075351715\n",
      "147 0.32191163301467896\n",
      "148 0.308621883392334\n",
      "149 0.29590684175491333\n",
      "150 0.2837298810482025\n",
      "151 0.2720790207386017\n",
      "152 0.2609221041202545\n",
      "153 0.2502516210079193\n",
      "154 0.24004380404949188\n",
      "155 0.23026885092258453\n",
      "156 0.2209109216928482\n",
      "157 0.21194399893283844\n",
      "158 0.20336145162582397\n",
      "159 0.19513998925685883\n",
      "160 0.1872694194316864\n",
      "161 0.17975102365016937\n",
      "162 0.1725393384695053\n",
      "163 0.16562248766422272\n",
      "164 0.1590043157339096\n",
      "165 0.15266932547092438\n",
      "166 0.1465904414653778\n",
      "167 0.140764981508255\n",
      "168 0.13518238067626953\n",
      "169 0.12983210384845734\n",
      "170 0.1247120276093483\n",
      "171 0.11980558186769485\n",
      "172 0.11510220170021057\n",
      "173 0.11059274524450302\n",
      "174 0.10626350343227386\n",
      "175 0.10211148858070374\n",
      "176 0.09812929481267929\n",
      "177 0.0943082720041275\n",
      "178 0.09065068513154984\n",
      "179 0.08713483810424805\n",
      "180 0.08376213908195496\n",
      "181 0.08052780479192734\n",
      "182 0.07741890102624893\n",
      "183 0.07443518191576004\n",
      "184 0.07157310098409653\n",
      "185 0.06882615387439728\n",
      "186 0.0661885067820549\n",
      "187 0.06365367025136948\n",
      "188 0.061220262199640274\n",
      "189 0.05888456106185913\n",
      "190 0.056639350950717926\n",
      "191 0.054484255611896515\n",
      "192 0.05241287872195244\n",
      "193 0.05042364075779915\n",
      "194 0.0485113151371479\n",
      "195 0.04667602851986885\n",
      "196 0.04491328448057175\n",
      "197 0.04322014003992081\n",
      "198 0.04159212484955788\n",
      "199 0.04002625122666359\n",
      "200 0.03852210193872452\n",
      "201 0.03707589581608772\n",
      "202 0.035685840994119644\n",
      "203 0.03435112163424492\n",
      "204 0.033066920936107635\n",
      "205 0.03183158114552498\n",
      "206 0.03064468689262867\n",
      "207 0.029504386708140373\n",
      "208 0.028407707810401917\n",
      "209 0.027352796867489815\n",
      "210 0.026337647810578346\n",
      "211 0.02536132000386715\n",
      "212 0.024422988295555115\n",
      "213 0.0235199723392725\n",
      "214 0.022651158273220062\n",
      "215 0.021815786138176918\n",
      "216 0.02101411111652851\n",
      "217 0.02024197392165661\n",
      "218 0.01949928142130375\n",
      "219 0.018784470856189728\n",
      "220 0.018097486346960068\n",
      "221 0.01743566058576107\n",
      "222 0.016798589378595352\n",
      "223 0.016186024993658066\n",
      "224 0.015596350654959679\n",
      "225 0.015028994530439377\n",
      "226 0.01448265090584755\n",
      "227 0.013956919312477112\n",
      "228 0.013451001606881618\n",
      "229 0.012963972985744476\n",
      "230 0.01249453704804182\n",
      "231 0.012042966671288013\n",
      "232 0.011608384549617767\n",
      "233 0.01118963398039341\n",
      "234 0.010787023231387138\n",
      "235 0.010398874059319496\n",
      "236 0.010024957358837128\n",
      "237 0.009664921090006828\n",
      "238 0.009318078868091106\n",
      "239 0.008984029293060303\n",
      "240 0.00866271834820509\n",
      "241 0.008353002369403839\n",
      "242 0.008054501377046108\n",
      "243 0.007767067290842533\n",
      "244 0.007490124087780714\n",
      "245 0.0072234091348946095\n",
      "246 0.006966326851397753\n",
      "247 0.006718732416629791\n",
      "248 0.006480066105723381\n",
      "249 0.006249978207051754\n",
      "250 0.006028245203197002\n",
      "251 0.005814739968627691\n",
      "252 0.0056088753044605255\n",
      "253 0.005410616751760244\n",
      "254 0.00521940179169178\n",
      "255 0.00503524811938405\n",
      "256 0.004857738967984915\n",
      "257 0.004686592612415552\n",
      "258 0.004521593451499939\n",
      "259 0.0043624830432236195\n",
      "260 0.0042091840878129005\n",
      "261 0.004061379469931126\n",
      "262 0.0039189038798213005\n",
      "263 0.00378162763081491\n",
      "264 0.0036491795908659697\n",
      "265 0.00352152599953115\n",
      "266 0.0033984039910137653\n",
      "267 0.00327964685857296\n",
      "268 0.003165149362757802\n",
      "269 0.003054741770029068\n",
      "270 0.0029483018442988396\n",
      "271 0.0028456407599151134\n",
      "272 0.0027466772589832544\n",
      "273 0.0026512222830206156\n",
      "274 0.002559088636189699\n",
      "275 0.0024702115915715694\n",
      "276 0.0023844835814088583\n",
      "277 0.0023018030915409327\n",
      "278 0.002222107257694006\n",
      "279 0.00214522541500628\n",
      "280 0.0020710204262286425\n",
      "281 0.0019994538743048906\n",
      "282 0.0019304604502394795\n",
      "283 0.0018638888141140342\n",
      "284 0.0017997183604165912\n",
      "285 0.0017378167249262333\n",
      "286 0.0016780354781076312\n",
      "287 0.0016203453997150064\n",
      "288 0.0015646865358576179\n",
      "289 0.0015110247768461704\n",
      "290 0.0014591764193028212\n",
      "291 0.0014091642806306481\n",
      "292 0.0013609066372737288\n",
      "293 0.001314336434006691\n",
      "294 0.0012693810276687145\n",
      "295 0.0012259819777682424\n",
      "296 0.0011841044761240482\n",
      "297 0.00114370824303478\n",
      "298 0.0011047155130654573\n",
      "299 0.001067074597813189\n",
      "300 0.0010307036573067307\n",
      "301 0.0009956302819773555\n",
      "302 0.0009617417235858738\n",
      "303 0.0009290552115999162\n",
      "304 0.0008975209202617407\n",
      "305 0.0008670442621223629\n",
      "306 0.0008376342011615634\n",
      "307 0.0008092288044281304\n",
      "308 0.000781793671194464\n",
      "309 0.0007553385803475976\n",
      "310 0.0007297853589989245\n",
      "311 0.0007051247521303594\n",
      "312 0.0006812954670749605\n",
      "313 0.0006582850473932922\n",
      "314 0.0006360617117024958\n",
      "315 0.0006146049709059298\n",
      "316 0.0005938828107900918\n",
      "317 0.0005738742765970528\n",
      "318 0.0005545464809983969\n",
      "319 0.000535879866220057\n",
      "320 0.00051785638788715\n",
      "321 0.0005004503764212132\n",
      "322 0.00048365601105615497\n",
      "323 0.00046741703408770263\n",
      "324 0.0004517370543908328\n",
      "325 0.00043658429058268666\n",
      "326 0.0004219521360937506\n",
      "327 0.0004078166384715587\n",
      "328 0.0003941604227293283\n",
      "329 0.00038097778451628983\n",
      "330 0.000368222565157339\n",
      "331 0.0003559231699910015\n",
      "332 0.00034404147299937904\n",
      "333 0.00033255654852837324\n",
      "334 0.00032145879231393337\n",
      "335 0.00031074907747097313\n",
      "336 0.000300387415336445\n",
      "337 0.0002903769491240382\n",
      "338 0.00028071278939023614\n",
      "339 0.00027137083816342056\n",
      "340 0.0002623477193992585\n",
      "341 0.0002536293468438089\n",
      "342 0.00024519849102944136\n",
      "343 0.00023705979401711375\n",
      "344 0.0002291919372510165\n",
      "345 0.00022159625950735062\n",
      "346 0.00021425051090773195\n",
      "347 0.0002071467461064458\n",
      "348 0.00020028457220178097\n",
      "349 0.00019365159096196294\n",
      "350 0.00018724035180639476\n",
      "351 0.00018105008348356932\n",
      "352 0.0001750649098539725\n",
      "353 0.0001692716614343226\n",
      "354 0.0001636869419598952\n",
      "355 0.0001582841359777376\n",
      "356 0.00015306592104025185\n",
      "357 0.00014801413635723293\n",
      "358 0.00014313524297904223\n",
      "359 0.0001384225906804204\n",
      "360 0.00013386746286414564\n",
      "361 0.0001294537796638906\n",
      "362 0.00012519414303824306\n",
      "363 0.00012107470683986321\n",
      "364 0.00011709555838024244\n",
      "365 0.00011324720981065184\n",
      "366 0.00010952723823720589\n",
      "367 0.00010593865590635687\n",
      "368 0.0001024604935082607\n",
      "369 9.910535300150514e-05\n",
      "370 9.584955114405602e-05\n",
      "371 9.271004819311202e-05\n",
      "372 8.96751371328719e-05\n",
      "373 8.673679258208722e-05\n",
      "374 8.389699360122904e-05\n",
      "375 8.11550416983664e-05\n",
      "376 7.850237307138741e-05\n",
      "377 7.593922782689333e-05\n",
      "378 7.345794438151643e-05\n",
      "379 7.105917029548436e-05\n",
      "380 6.873586971778423e-05\n",
      "381 6.649253191426396e-05\n",
      "382 6.43228049739264e-05\n",
      "383 6.222687079571187e-05\n",
      "384 6.0197759012226015e-05\n",
      "385 5.823918036185205e-05\n",
      "386 5.6342283642152324e-05\n",
      "387 5.451011747936718e-05\n",
      "388 5.273701754049398e-05\n",
      "389 5.101859642309137e-05\n",
      "390 4.936078767059371e-05\n",
      "391 4.7756035201018676e-05\n",
      "392 4.620589970727451e-05\n",
      "393 4.470311978366226e-05\n",
      "394 4.32562519563362e-05\n",
      "395 4.1850820707622916e-05\n",
      "396 4.0492181142326444e-05\n",
      "397 3.917660433216952e-05\n",
      "398 3.790900518652052e-05\n",
      "399 3.668220597319305e-05\n",
      "400 3.549260145518929e-05\n",
      "401 3.434446261962876e-05\n",
      "402 3.323376222397201e-05\n",
      "403 3.2158251997316256e-05\n",
      "404 3.111785918008536e-05\n",
      "405 3.0111052183201537e-05\n",
      "406 2.913966636697296e-05\n",
      "407 2.8200778615428135e-05\n",
      "408 2.728763320192229e-05\n",
      "409 2.6408093617646955e-05\n",
      "410 2.5556524633429945e-05\n",
      "411 2.4729712094995193e-05\n",
      "412 2.3932556359795853e-05\n",
      "413 2.316260179213714e-05\n",
      "414 2.241476067865733e-05\n",
      "415 2.1694439055863768e-05\n",
      "416 2.0994584701838903e-05\n",
      "417 2.0319284885772504e-05\n",
      "418 1.966662057384383e-05\n",
      "419 1.9032609998248518e-05\n",
      "420 1.8419104890199378e-05\n",
      "421 1.7827611372922547e-05\n",
      "422 1.7253594705834985e-05\n",
      "423 1.67002908710856e-05\n",
      "424 1.6165527995326556e-05\n",
      "425 1.564571721246466e-05\n",
      "426 1.5143106793402694e-05\n",
      "427 1.4656885468866676e-05\n",
      "428 1.418606552761048e-05\n",
      "429 1.3733398191106971e-05\n",
      "430 1.3292617950355634e-05\n",
      "431 1.2865256394434255e-05\n",
      "432 1.2453746421670076e-05\n",
      "433 1.2055897059326526e-05\n",
      "434 1.1669336345221382e-05\n",
      "435 1.129667907662224e-05\n",
      "436 1.0935571481240913e-05\n",
      "437 1.0586041753413156e-05\n",
      "438 1.0247120371786878e-05\n",
      "439 9.919333933794405e-06\n",
      "440 9.601561941963155e-06\n",
      "441 9.29584803088801e-06\n",
      "442 8.998910743684974e-06\n",
      "443 8.711792361282278e-06\n",
      "444 8.434180017502513e-06\n",
      "445 8.16456122265663e-06\n",
      "446 7.90570629760623e-06\n",
      "447 7.652192834939342e-06\n",
      "448 7.407975317619275e-06\n",
      "449 7.170985099946847e-06\n",
      "450 6.943346306798048e-06\n",
      "451 6.723039405187592e-06\n",
      "452 6.509560535050696e-06\n",
      "453 6.301041139522567e-06\n",
      "454 6.101649432821432e-06\n",
      "455 5.907395461690612e-06\n",
      "456 5.718633929063799e-06\n",
      "457 5.537117431231309e-06\n",
      "458 5.36157267561066e-06\n",
      "459 5.1905340114899445e-06\n",
      "460 5.026045528211398e-06\n",
      "461 4.866049039264908e-06\n",
      "462 4.711064775619889e-06\n",
      "463 4.561633431876544e-06\n",
      "464 4.41731208411511e-06\n",
      "465 4.27804525315878e-06\n",
      "466 4.142098987358622e-06\n",
      "467 4.010512384411413e-06\n",
      "468 3.8830221456009895e-06\n",
      "469 3.7604390854539815e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470 3.6409585391083965e-06\n",
      "471 3.5260416098026326e-06\n",
      "472 3.4132888231397374e-06\n",
      "473 3.3061205613194034e-06\n",
      "474 3.201116442141938e-06\n",
      "475 3.1005440632725367e-06\n",
      "476 3.001604909513844e-06\n",
      "477 2.9067341529298574e-06\n",
      "478 2.8154388473922154e-06\n",
      "479 2.7262694857199676e-06\n",
      "480 2.6399663966003573e-06\n",
      "481 2.5568981527612777e-06\n",
      "482 2.4758132894930895e-06\n",
      "483 2.3980633159226272e-06\n",
      "484 2.322020009160042e-06\n",
      "485 2.24806376536435e-06\n",
      "486 2.177251190005336e-06\n",
      "487 2.1092737370054238e-06\n",
      "488 2.0423960904736305e-06\n",
      "489 1.9779051854129648e-06\n",
      "490 1.9154119854647433e-06\n",
      "491 1.8556474969955161e-06\n",
      "492 1.7966224277188303e-06\n",
      "493 1.740423158480553e-06\n",
      "494 1.685539132267877e-06\n",
      "495 1.632676571716729e-06\n",
      "496 1.5816104905752582e-06\n",
      "497 1.531224484097038e-06\n",
      "498 1.4830255850029062e-06\n",
      "499 1.4364138678502059e-06\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cuba/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/Users/cuba/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/15], Step [10/469], Reconst Loss: 35607.0117, KL Div: 3252.1628\n",
      "Epoch[1/15], Step [20/469], Reconst Loss: 30020.0273, KL Div: 1089.5189\n",
      "Epoch[1/15], Step [30/469], Reconst Loss: 27580.6211, KL Div: 1174.6149\n",
      "Epoch[1/15], Step [40/469], Reconst Loss: 26698.7949, KL Div: 645.2008\n",
      "Epoch[1/15], Step [50/469], Reconst Loss: 26766.0840, KL Div: 617.2636\n",
      "Epoch[1/15], Step [60/469], Reconst Loss: 26262.5547, KL Div: 700.7596\n",
      "Epoch[1/15], Step [70/469], Reconst Loss: 25164.1992, KL Div: 808.3029\n",
      "Epoch[1/15], Step [80/469], Reconst Loss: 24524.2715, KL Div: 857.1606\n",
      "Epoch[1/15], Step [90/469], Reconst Loss: 23485.5879, KL Div: 1264.2985\n",
      "Epoch[1/15], Step [100/469], Reconst Loss: 22249.1855, KL Div: 1290.5806\n",
      "Epoch[1/15], Step [110/469], Reconst Loss: 22326.3262, KL Div: 1375.0464\n",
      "Epoch[1/15], Step [120/469], Reconst Loss: 20346.1992, KL Div: 1678.9233\n",
      "Epoch[1/15], Step [130/469], Reconst Loss: 20621.3887, KL Div: 1729.7217\n",
      "Epoch[1/15], Step [140/469], Reconst Loss: 19616.2246, KL Div: 1887.4272\n",
      "Epoch[1/15], Step [150/469], Reconst Loss: 19194.0059, KL Div: 1822.1338\n",
      "Epoch[1/15], Step [160/469], Reconst Loss: 19477.4297, KL Div: 1776.2180\n",
      "Epoch[1/15], Step [170/469], Reconst Loss: 19260.9043, KL Div: 1828.0720\n",
      "Epoch[1/15], Step [180/469], Reconst Loss: 18650.6465, KL Div: 1897.3750\n",
      "Epoch[1/15], Step [190/469], Reconst Loss: 17920.9375, KL Div: 1947.9773\n",
      "Epoch[1/15], Step [200/469], Reconst Loss: 17722.7109, KL Div: 1989.5253\n",
      "Epoch[1/15], Step [210/469], Reconst Loss: 17583.5449, KL Div: 2022.5092\n",
      "Epoch[1/15], Step [220/469], Reconst Loss: 17285.5000, KL Div: 2095.9731\n",
      "Epoch[1/15], Step [230/469], Reconst Loss: 17171.7578, KL Div: 1953.5095\n",
      "Epoch[1/15], Step [240/469], Reconst Loss: 16567.3613, KL Div: 2128.6394\n",
      "Epoch[1/15], Step [250/469], Reconst Loss: 16749.5605, KL Div: 2012.9102\n",
      "Epoch[1/15], Step [260/469], Reconst Loss: 16919.3535, KL Div: 2135.0752\n",
      "Epoch[1/15], Step [270/469], Reconst Loss: 16170.3174, KL Div: 2274.7017\n",
      "Epoch[1/15], Step [280/469], Reconst Loss: 16210.8926, KL Div: 2275.2305\n",
      "Epoch[1/15], Step [290/469], Reconst Loss: 16091.4814, KL Div: 2355.8555\n",
      "Epoch[1/15], Step [300/469], Reconst Loss: 15245.9648, KL Div: 2301.4595\n",
      "Epoch[1/15], Step [310/469], Reconst Loss: 16086.0244, KL Div: 2281.6875\n",
      "Epoch[1/15], Step [320/469], Reconst Loss: 15655.2812, KL Div: 2365.0408\n",
      "Epoch[1/15], Step [330/469], Reconst Loss: 15414.0439, KL Div: 2453.2063\n",
      "Epoch[1/15], Step [340/469], Reconst Loss: 14896.6260, KL Div: 2401.4377\n",
      "Epoch[1/15], Step [350/469], Reconst Loss: 15394.2539, KL Div: 2648.7974\n",
      "Epoch[1/15], Step [360/469], Reconst Loss: 14991.0547, KL Div: 2446.7754\n",
      "Epoch[1/15], Step [370/469], Reconst Loss: 14776.7100, KL Div: 2496.0723\n",
      "Epoch[1/15], Step [380/469], Reconst Loss: 15046.8281, KL Div: 2574.5664\n",
      "Epoch[1/15], Step [390/469], Reconst Loss: 14322.5479, KL Div: 2453.4392\n",
      "Epoch[1/15], Step [400/469], Reconst Loss: 15121.3047, KL Div: 2492.4932\n",
      "Epoch[1/15], Step [410/469], Reconst Loss: 14289.0820, KL Div: 2492.1167\n",
      "Epoch[1/15], Step [420/469], Reconst Loss: 13941.5137, KL Div: 2549.8687\n",
      "Epoch[1/15], Step [430/469], Reconst Loss: 14070.7715, KL Div: 2549.5723\n",
      "Epoch[1/15], Step [440/469], Reconst Loss: 14511.5400, KL Div: 2521.9119\n",
      "Epoch[1/15], Step [450/469], Reconst Loss: 14138.3906, KL Div: 2513.0793\n",
      "Epoch[1/15], Step [460/469], Reconst Loss: 13909.7393, KL Div: 2555.2471\n",
      "Epoch[2/15], Step [10/469], Reconst Loss: 13885.6777, KL Div: 2667.7729\n",
      "Epoch[2/15], Step [20/469], Reconst Loss: 14173.3643, KL Div: 2505.9685\n",
      "Epoch[2/15], Step [30/469], Reconst Loss: 13558.1758, KL Div: 2690.5818\n",
      "Epoch[2/15], Step [40/469], Reconst Loss: 14082.0557, KL Div: 2668.2041\n",
      "Epoch[2/15], Step [50/469], Reconst Loss: 14239.2900, KL Div: 2764.4033\n",
      "Epoch[2/15], Step [60/469], Reconst Loss: 13287.6836, KL Div: 2659.0984\n",
      "Epoch[2/15], Step [70/469], Reconst Loss: 13229.8574, KL Div: 2703.3091\n",
      "Epoch[2/15], Step [80/469], Reconst Loss: 13365.5566, KL Div: 2642.6731\n",
      "Epoch[2/15], Step [90/469], Reconst Loss: 13669.9824, KL Div: 2665.9207\n",
      "Epoch[2/15], Step [100/469], Reconst Loss: 13285.7852, KL Div: 2674.3164\n",
      "Epoch[2/15], Step [110/469], Reconst Loss: 12956.0352, KL Div: 2636.1101\n",
      "Epoch[2/15], Step [120/469], Reconst Loss: 13247.1201, KL Div: 2685.0371\n",
      "Epoch[2/15], Step [130/469], Reconst Loss: 13333.3438, KL Div: 2821.6562\n",
      "Epoch[2/15], Step [140/469], Reconst Loss: 13327.9473, KL Div: 2749.8750\n",
      "Epoch[2/15], Step [150/469], Reconst Loss: 13433.1445, KL Div: 2851.9116\n",
      "Epoch[2/15], Step [160/469], Reconst Loss: 12519.6104, KL Div: 2614.1082\n",
      "Epoch[2/15], Step [170/469], Reconst Loss: 12251.8105, KL Div: 2734.5515\n",
      "Epoch[2/15], Step [180/469], Reconst Loss: 12819.3574, KL Div: 2734.6548\n",
      "Epoch[2/15], Step [190/469], Reconst Loss: 13003.9004, KL Div: 2789.7712\n",
      "Epoch[2/15], Step [200/469], Reconst Loss: 13087.6211, KL Div: 2785.5713\n",
      "Epoch[2/15], Step [210/469], Reconst Loss: 13461.8105, KL Div: 2701.2522\n",
      "Epoch[2/15], Step [220/469], Reconst Loss: 12719.4004, KL Div: 2763.7014\n",
      "Epoch[2/15], Step [230/469], Reconst Loss: 13442.7354, KL Div: 2742.8140\n",
      "Epoch[2/15], Step [240/469], Reconst Loss: 13105.4131, KL Div: 2873.6458\n",
      "Epoch[2/15], Step [250/469], Reconst Loss: 12399.3779, KL Div: 2943.6189\n",
      "Epoch[2/15], Step [260/469], Reconst Loss: 12258.6660, KL Div: 2689.6323\n",
      "Epoch[2/15], Step [270/469], Reconst Loss: 12891.1367, KL Div: 2858.2544\n",
      "Epoch[2/15], Step [280/469], Reconst Loss: 12139.3281, KL Div: 2875.1414\n",
      "Epoch[2/15], Step [290/469], Reconst Loss: 12278.7715, KL Div: 2827.1777\n",
      "Epoch[2/15], Step [300/469], Reconst Loss: 12781.5225, KL Div: 2747.2808\n",
      "Epoch[2/15], Step [310/469], Reconst Loss: 13440.4912, KL Div: 2813.7026\n",
      "Epoch[2/15], Step [320/469], Reconst Loss: 12617.6592, KL Div: 2871.2283\n",
      "Epoch[2/15], Step [330/469], Reconst Loss: 12645.9990, KL Div: 2870.3120\n",
      "Epoch[2/15], Step [340/469], Reconst Loss: 12228.1787, KL Div: 2876.1667\n",
      "Epoch[2/15], Step [350/469], Reconst Loss: 12484.0986, KL Div: 2797.4915\n",
      "Epoch[2/15], Step [360/469], Reconst Loss: 12648.2295, KL Div: 2925.4409\n",
      "Epoch[2/15], Step [370/469], Reconst Loss: 12958.8516, KL Div: 2914.6328\n",
      "Epoch[2/15], Step [380/469], Reconst Loss: 12825.1416, KL Div: 2849.7119\n",
      "Epoch[2/15], Step [390/469], Reconst Loss: 12198.4854, KL Div: 2807.8262\n",
      "Epoch[2/15], Step [400/469], Reconst Loss: 12440.2754, KL Div: 2958.3530\n",
      "Epoch[2/15], Step [410/469], Reconst Loss: 12266.4092, KL Div: 2956.5076\n",
      "Epoch[2/15], Step [420/469], Reconst Loss: 12265.1240, KL Div: 2896.3494\n",
      "Epoch[2/15], Step [430/469], Reconst Loss: 12189.6982, KL Div: 3058.3237\n",
      "Epoch[2/15], Step [440/469], Reconst Loss: 11715.0332, KL Div: 2863.5708\n",
      "Epoch[2/15], Step [450/469], Reconst Loss: 11902.8066, KL Div: 2998.1040\n",
      "Epoch[2/15], Step [460/469], Reconst Loss: 12905.9180, KL Div: 3025.1218\n",
      "Epoch[3/15], Step [10/469], Reconst Loss: 12375.9609, KL Div: 2951.3323\n",
      "Epoch[3/15], Step [20/469], Reconst Loss: 12624.8457, KL Div: 2906.4314\n",
      "Epoch[3/15], Step [30/469], Reconst Loss: 11974.2275, KL Div: 2987.4402\n",
      "Epoch[3/15], Step [40/469], Reconst Loss: 11732.8652, KL Div: 2920.2141\n",
      "Epoch[3/15], Step [50/469], Reconst Loss: 12310.3770, KL Div: 3112.7910\n",
      "Epoch[3/15], Step [60/469], Reconst Loss: 12602.9785, KL Div: 3041.7117\n",
      "Epoch[3/15], Step [70/469], Reconst Loss: 11818.4453, KL Div: 2947.7119\n",
      "Epoch[3/15], Step [80/469], Reconst Loss: 11662.5908, KL Div: 2985.1238\n",
      "Epoch[3/15], Step [90/469], Reconst Loss: 11551.7529, KL Div: 2942.8843\n",
      "Epoch[3/15], Step [100/469], Reconst Loss: 11625.9736, KL Div: 2939.2852\n",
      "Epoch[3/15], Step [110/469], Reconst Loss: 12226.2930, KL Div: 2989.2881\n",
      "Epoch[3/15], Step [120/469], Reconst Loss: 12236.3936, KL Div: 2949.2195\n",
      "Epoch[3/15], Step [130/469], Reconst Loss: 12443.8037, KL Div: 2962.8655\n",
      "Epoch[3/15], Step [140/469], Reconst Loss: 11339.7705, KL Div: 3014.1548\n",
      "Epoch[3/15], Step [150/469], Reconst Loss: 11736.7754, KL Div: 2874.1301\n",
      "Epoch[3/15], Step [160/469], Reconst Loss: 11904.4072, KL Div: 3134.1497\n",
      "Epoch[3/15], Step [170/469], Reconst Loss: 11873.1309, KL Div: 3008.7834\n",
      "Epoch[3/15], Step [180/469], Reconst Loss: 11665.9033, KL Div: 3010.5181\n",
      "Epoch[3/15], Step [190/469], Reconst Loss: 11798.7568, KL Div: 3080.8228\n",
      "Epoch[3/15], Step [200/469], Reconst Loss: 11485.4277, KL Div: 2887.8093\n",
      "Epoch[3/15], Step [210/469], Reconst Loss: 12213.1348, KL Div: 3061.4626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[3/15], Step [220/469], Reconst Loss: 11947.2988, KL Div: 3039.6143\n",
      "Epoch[3/15], Step [230/469], Reconst Loss: 12028.2783, KL Div: 3044.8308\n",
      "Epoch[3/15], Step [240/469], Reconst Loss: 11262.1514, KL Div: 2913.2280\n",
      "Epoch[3/15], Step [250/469], Reconst Loss: 11590.3047, KL Div: 3048.4224\n",
      "Epoch[3/15], Step [260/469], Reconst Loss: 11555.4795, KL Div: 3061.5991\n",
      "Epoch[3/15], Step [270/469], Reconst Loss: 11841.4736, KL Div: 2988.6853\n",
      "Epoch[3/15], Step [280/469], Reconst Loss: 11411.2979, KL Div: 3004.0178\n",
      "Epoch[3/15], Step [290/469], Reconst Loss: 11631.3428, KL Div: 3039.4556\n",
      "Epoch[3/15], Step [300/469], Reconst Loss: 11496.9375, KL Div: 3139.2642\n",
      "Epoch[3/15], Step [310/469], Reconst Loss: 11533.1143, KL Div: 3017.4001\n",
      "Epoch[3/15], Step [320/469], Reconst Loss: 11960.7695, KL Div: 3145.2212\n",
      "Epoch[3/15], Step [330/469], Reconst Loss: 11250.1475, KL Div: 2952.7268\n",
      "Epoch[3/15], Step [340/469], Reconst Loss: 11622.8809, KL Div: 3061.2671\n",
      "Epoch[3/15], Step [350/469], Reconst Loss: 11316.8857, KL Div: 3051.7356\n",
      "Epoch[3/15], Step [360/469], Reconst Loss: 11596.1973, KL Div: 3113.6653\n",
      "Epoch[3/15], Step [370/469], Reconst Loss: 11535.3086, KL Div: 3017.3586\n",
      "Epoch[3/15], Step [380/469], Reconst Loss: 11379.7451, KL Div: 2975.4619\n",
      "Epoch[3/15], Step [390/469], Reconst Loss: 11267.6924, KL Div: 3057.4329\n",
      "Epoch[3/15], Step [400/469], Reconst Loss: 11492.3652, KL Div: 2973.3005\n",
      "Epoch[3/15], Step [410/469], Reconst Loss: 11754.5088, KL Div: 3115.9543\n",
      "Epoch[3/15], Step [420/469], Reconst Loss: 11184.9922, KL Div: 3078.7207\n",
      "Epoch[3/15], Step [430/469], Reconst Loss: 11284.5000, KL Div: 3120.3770\n",
      "Epoch[3/15], Step [440/469], Reconst Loss: 11403.7422, KL Div: 3030.2664\n",
      "Epoch[3/15], Step [450/469], Reconst Loss: 11186.6768, KL Div: 3047.6917\n",
      "Epoch[3/15], Step [460/469], Reconst Loss: 11444.7988, KL Div: 3142.8066\n",
      "Epoch[4/15], Step [10/469], Reconst Loss: 10871.3604, KL Div: 3132.9836\n",
      "Epoch[4/15], Step [20/469], Reconst Loss: 11146.7227, KL Div: 2930.4998\n",
      "Epoch[4/15], Step [30/469], Reconst Loss: 11430.5088, KL Div: 3131.3591\n",
      "Epoch[4/15], Step [40/469], Reconst Loss: 11395.4756, KL Div: 3026.2375\n",
      "Epoch[4/15], Step [50/469], Reconst Loss: 10770.0908, KL Div: 3066.9829\n",
      "Epoch[4/15], Step [60/469], Reconst Loss: 11160.5752, KL Div: 3134.8777\n",
      "Epoch[4/15], Step [70/469], Reconst Loss: 11051.8037, KL Div: 3025.5078\n",
      "Epoch[4/15], Step [80/469], Reconst Loss: 11376.4834, KL Div: 3053.0823\n",
      "Epoch[4/15], Step [90/469], Reconst Loss: 11564.7871, KL Div: 3159.1333\n",
      "Epoch[4/15], Step [100/469], Reconst Loss: 11031.7129, KL Div: 3122.1079\n",
      "Epoch[4/15], Step [110/469], Reconst Loss: 11173.1387, KL Div: 3027.4749\n",
      "Epoch[4/15], Step [120/469], Reconst Loss: 11043.0186, KL Div: 3038.9829\n",
      "Epoch[4/15], Step [130/469], Reconst Loss: 10743.2051, KL Div: 3027.4900\n",
      "Epoch[4/15], Step [140/469], Reconst Loss: 11695.4492, KL Div: 3101.1782\n",
      "Epoch[4/15], Step [150/469], Reconst Loss: 11100.1396, KL Div: 3067.0623\n",
      "Epoch[4/15], Step [160/469], Reconst Loss: 11612.8174, KL Div: 3049.0664\n",
      "Epoch[4/15], Step [170/469], Reconst Loss: 11469.0615, KL Div: 3054.2300\n",
      "Epoch[4/15], Step [180/469], Reconst Loss: 11511.0654, KL Div: 3249.4048\n",
      "Epoch[4/15], Step [190/469], Reconst Loss: 10965.0889, KL Div: 3074.0525\n",
      "Epoch[4/15], Step [200/469], Reconst Loss: 10769.1846, KL Div: 3055.0540\n",
      "Epoch[4/15], Step [210/469], Reconst Loss: 10982.7676, KL Div: 3141.6738\n",
      "Epoch[4/15], Step [220/469], Reconst Loss: 12009.6152, KL Div: 3191.9978\n",
      "Epoch[4/15], Step [230/469], Reconst Loss: 11778.8555, KL Div: 3021.7954\n",
      "Epoch[4/15], Step [240/469], Reconst Loss: 10794.2207, KL Div: 3065.8391\n",
      "Epoch[4/15], Step [250/469], Reconst Loss: 10847.3232, KL Div: 3122.1455\n",
      "Epoch[4/15], Step [260/469], Reconst Loss: 10783.8154, KL Div: 3179.5796\n",
      "Epoch[4/15], Step [270/469], Reconst Loss: 11277.8770, KL Div: 3155.1379\n",
      "Epoch[4/15], Step [280/469], Reconst Loss: 11031.6426, KL Div: 3060.1868\n",
      "Epoch[4/15], Step [290/469], Reconst Loss: 10874.9814, KL Div: 3112.9351\n",
      "Epoch[4/15], Step [300/469], Reconst Loss: 11197.8105, KL Div: 3230.7861\n",
      "Epoch[4/15], Step [310/469], Reconst Loss: 11153.5537, KL Div: 3193.1147\n",
      "Epoch[4/15], Step [320/469], Reconst Loss: 11408.1709, KL Div: 3060.3999\n",
      "Epoch[4/15], Step [330/469], Reconst Loss: 10740.8516, KL Div: 3181.1470\n",
      "Epoch[4/15], Step [340/469], Reconst Loss: 11237.2178, KL Div: 3000.3455\n",
      "Epoch[4/15], Step [350/469], Reconst Loss: 11921.6719, KL Div: 3213.3469\n",
      "Epoch[4/15], Step [360/469], Reconst Loss: 11187.6699, KL Div: 3211.9604\n",
      "Epoch[4/15], Step [370/469], Reconst Loss: 11098.9883, KL Div: 3053.6680\n",
      "Epoch[4/15], Step [380/469], Reconst Loss: 10677.4561, KL Div: 3173.0356\n",
      "Epoch[4/15], Step [390/469], Reconst Loss: 11467.9482, KL Div: 3065.6941\n",
      "Epoch[4/15], Step [400/469], Reconst Loss: 10785.7822, KL Div: 3079.2092\n",
      "Epoch[4/15], Step [410/469], Reconst Loss: 11377.1436, KL Div: 3119.2339\n",
      "Epoch[4/15], Step [420/469], Reconst Loss: 11361.6006, KL Div: 3213.1211\n",
      "Epoch[4/15], Step [430/469], Reconst Loss: 11060.6143, KL Div: 3097.8879\n",
      "Epoch[4/15], Step [440/469], Reconst Loss: 11157.2744, KL Div: 3121.0017\n",
      "Epoch[4/15], Step [450/469], Reconst Loss: 11004.2510, KL Div: 3110.5388\n",
      "Epoch[4/15], Step [460/469], Reconst Loss: 11180.8730, KL Div: 3210.3701\n",
      "Epoch[5/15], Step [10/469], Reconst Loss: 11414.5996, KL Div: 3094.0315\n",
      "Epoch[5/15], Step [20/469], Reconst Loss: 11314.6289, KL Div: 3094.4927\n",
      "Epoch[5/15], Step [30/469], Reconst Loss: 11017.1367, KL Div: 3118.5034\n",
      "Epoch[5/15], Step [40/469], Reconst Loss: 10966.5518, KL Div: 3158.6482\n",
      "Epoch[5/15], Step [50/469], Reconst Loss: 11201.4443, KL Div: 3079.9331\n",
      "Epoch[5/15], Step [60/469], Reconst Loss: 10806.1738, KL Div: 3174.9065\n",
      "Epoch[5/15], Step [70/469], Reconst Loss: 10566.0645, KL Div: 3016.3997\n",
      "Epoch[5/15], Step [80/469], Reconst Loss: 10837.6670, KL Div: 3180.5854\n",
      "Epoch[5/15], Step [90/469], Reconst Loss: 11480.6357, KL Div: 3145.4495\n",
      "Epoch[5/15], Step [100/469], Reconst Loss: 11118.8262, KL Div: 3199.3853\n",
      "Epoch[5/15], Step [110/469], Reconst Loss: 10968.2588, KL Div: 3171.2058\n",
      "Epoch[5/15], Step [120/469], Reconst Loss: 11505.5186, KL Div: 3181.1079\n",
      "Epoch[5/15], Step [130/469], Reconst Loss: 10931.2861, KL Div: 3069.2554\n",
      "Epoch[5/15], Step [140/469], Reconst Loss: 10236.1162, KL Div: 3134.6047\n",
      "Epoch[5/15], Step [150/469], Reconst Loss: 10618.8203, KL Div: 3079.2014\n",
      "Epoch[5/15], Step [160/469], Reconst Loss: 11142.8906, KL Div: 3070.1077\n",
      "Epoch[5/15], Step [170/469], Reconst Loss: 10555.7920, KL Div: 3096.7788\n",
      "Epoch[5/15], Step [180/469], Reconst Loss: 11065.6182, KL Div: 3109.7864\n",
      "Epoch[5/15], Step [190/469], Reconst Loss: 10775.7490, KL Div: 3149.9119\n",
      "Epoch[5/15], Step [200/469], Reconst Loss: 10536.9443, KL Div: 3032.4409\n",
      "Epoch[5/15], Step [210/469], Reconst Loss: 10863.6260, KL Div: 3119.1392\n",
      "Epoch[5/15], Step [220/469], Reconst Loss: 10767.8779, KL Div: 3187.4807\n",
      "Epoch[5/15], Step [230/469], Reconst Loss: 10897.3877, KL Div: 3197.7642\n",
      "Epoch[5/15], Step [240/469], Reconst Loss: 11350.2910, KL Div: 3153.5012\n",
      "Epoch[5/15], Step [250/469], Reconst Loss: 10981.4219, KL Div: 3146.1853\n",
      "Epoch[5/15], Step [260/469], Reconst Loss: 10992.5137, KL Div: 3053.7490\n",
      "Epoch[5/15], Step [270/469], Reconst Loss: 11382.3711, KL Div: 3205.0100\n",
      "Epoch[5/15], Step [280/469], Reconst Loss: 10588.2314, KL Div: 3091.0264\n",
      "Epoch[5/15], Step [290/469], Reconst Loss: 10985.5967, KL Div: 3259.5242\n",
      "Epoch[5/15], Step [300/469], Reconst Loss: 10561.8535, KL Div: 3064.6028\n",
      "Epoch[5/15], Step [310/469], Reconst Loss: 11395.5508, KL Div: 3274.5215\n",
      "Epoch[5/15], Step [320/469], Reconst Loss: 10570.0615, KL Div: 3075.6904\n",
      "Epoch[5/15], Step [330/469], Reconst Loss: 10930.4160, KL Div: 3090.6042\n",
      "Epoch[5/15], Step [340/469], Reconst Loss: 10462.6201, KL Div: 3107.5652\n",
      "Epoch[5/15], Step [350/469], Reconst Loss: 11275.6436, KL Div: 3214.5071\n",
      "Epoch[5/15], Step [360/469], Reconst Loss: 10453.8789, KL Div: 3039.9878\n",
      "Epoch[5/15], Step [370/469], Reconst Loss: 11392.9062, KL Div: 3104.3796\n",
      "Epoch[5/15], Step [380/469], Reconst Loss: 11185.1875, KL Div: 3164.8167\n",
      "Epoch[5/15], Step [390/469], Reconst Loss: 10901.5703, KL Div: 3221.2556\n",
      "Epoch[5/15], Step [400/469], Reconst Loss: 10661.8594, KL Div: 3233.2373\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-92abf557e37b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cuba/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cuba/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cuba/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/torchvision-0.1.9-py3.6.egg/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cuba/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2282\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mversionadded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.1\u001b[0m\u001b[0;36m.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2283\u001b[0m     \"\"\"\n\u001b[0;32m-> 2284\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array_interface__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2285\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shape'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2286\u001b[0m     \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create a directory if not exists\n",
    "sample_dir = 'samples'\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)\n",
    "\n",
    "# Hyper-parameters\n",
    "image_size = 784\n",
    "h_dim = 400\n",
    "z_dim = 20\n",
    "num_epochs = 15\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# MNIST dataset\n",
    "dataset = torchvision.datasets.MNIST(root='../../data',\n",
    "                                     train=True,\n",
    "                                     transform=transforms.ToTensor(),\n",
    "                                     download=True)\n",
    "\n",
    "# Data loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True)\n",
    "\n",
    "\n",
    "# VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size=784, h_dim=400, z_dim=20):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(image_size, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim)\n",
    "        self.fc5 = nn.Linear(h_dim, image_size)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        return self.fc2(h), self.fc3(h)\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(log_var/2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        return F.sigmoid(self.fc5(h))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_reconst = self.decode(z)\n",
    "        return x_reconst, mu, log_var\n",
    "\n",
    "model = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Start training\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x, _) in enumerate(data_loader):\n",
    "        # Forward pass\n",
    "        x = x.to(device).view(-1, image_size)\n",
    "        x_reconst, mu, log_var = model(x)\n",
    "        \n",
    "        # Compute reconstruction loss and kl divergence\n",
    "        # For KL divergence, see Appendix B in VAE paper or http://yunjey47.tistory.com/43\n",
    "        reconst_loss = F.binary_cross_entropy(x_reconst, x, size_average=False)\n",
    "        kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        loss = reconst_loss + kl_div\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 10 == 0:\n",
    "            print (\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}\" \n",
    "                   .format(epoch+1, num_epochs, i+1, len(data_loader), reconst_loss.item(), kl_div.item()))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Save the sampled images\n",
    "        z = torch.randn(batch_size, z_dim).to(device)\n",
    "        out = model.decode(z).view(-1, 1, 28, 28)\n",
    "        save_image(out, os.path.join(sample_dir, 'sampled-{}.png'.format(epoch+1)))\n",
    "\n",
    "        # Save the reconstructed images\n",
    "        out, _, _ = model(x)\n",
    "        x_concat = torch.cat([x.view(-1, 1, 28, 28), out.view(-1, 1, 28, 28)], dim=3)\n",
    "        save_image(x_concat, os.path.join(sample_dir, 'reconst-{}.png'.format(epoch+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
